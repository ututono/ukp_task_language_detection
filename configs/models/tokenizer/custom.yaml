# @package tokenizer
encoder_type: "custom"
encoding_type: "char"
vocab_size: 1000
max_length: 200